# Introduction to Session

In the previous session, you learnt that Bag of Words does not capture the meaning of words. You also learnt how to capture the geometric representation of meaning in the king, queen, man and woman examples. In this session, you will learn about Word2Vec algorithms that will create word vectors by capturing the meaning.

**VIDEO**

To break it down, this session will cover the following topics:

1.  Intuition of Word2Vec models: the Continous Bag of Words (CBOW) model and the Skipgram model
    
2.  Training input data for CBOW model
    
3.  Architecture of neural network and training of CBOW model
    
4.  Differences between the Skipgram model and the CBOW model 
    
5.  Code demonstration for creating word vectors using the Gensim library
    

People you will hear from in this session

Adjunct Faculty

[Jaidev Deshpande](http://www.linkedin.com/in/jaidevd/)

Principal Data Scientist at Gramener

In the next segment, you will learn how the Word2vec model works and understand the end goal of this model before going into details on how it works.
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dEniteeTHbjH",
        "aZAFPeCr66Wm",
        "5Tgoxk2BcdBy",
        "y_rAhbB7GlIq"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YZol8iJCto-"
      },
      "source": [
        "# **Library and Dataset Import**\n",
        "\n",
        "The examples uses tensorflow2.x for NLP Modelling. \n",
        "\n",
        "The indic-nlp-library is used for tokenization.\n",
        "\n",
        "Dataset used is for English Hindi translation, however can be easily adopted for 10 other major Indian language as available [here](http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/) or for any other language pair (adopt as per the tensorflow tutorial references provided below). "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoWjWlMckq4V",
        "outputId": "b1eaed12-b53c-4bbc-ae96-0d9ca679b8d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 184 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 204 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 215 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 225 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 235 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 245 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 266 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 276 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 286 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 296 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 307 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 317 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 327 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 348 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 358 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 368 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 378 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 389 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 399 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 409 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 419 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 430 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 440 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 450 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 460 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 471 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 481 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 491 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 501 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 512 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 522 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 532 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 542 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 552 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 563 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 573 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 583 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 593 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 604 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 614 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 624 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 634 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 645 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 655 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 665 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 675 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 686 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 696 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 706 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 716 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 727 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 737 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 747 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 757 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 768 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 778 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 788 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 798 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 808 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 819 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 829 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 839 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 849 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 860 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 870 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 880 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 890 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 901 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 911 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 921 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 931 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 942 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 952 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 962 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 972 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 983 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 993 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.0 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.6 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 14.8 MB/s \n",
            "\u001b[?25htime: 1.32 ms (started: 2022-11-03 22:20:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO8KYb6yYphB",
        "outputId": "60fa34a5-8c73-4c50-a705-6d673a7c328a"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os \n",
        "import io\n",
        "import time\n",
        "!pip3 install -q indic-nlp-library"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 40 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 26.3 MB/s \n",
            "\u001b[?25htime: 12.3 s (started: 2022-11-03 22:20:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qouiinSUbKUd",
        "outputId": "c885d51e-b090-4f38-83cc-906e2d88a4ac"
      },
      "source": [
        "!wget \"http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/indic_languages_corpus.tar.gz\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-03 22:20:53--  http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/indic_languages_corpus.tar.gz\n",
            "Resolving lotus.kuee.kyoto-u.ac.jp (lotus.kuee.kyoto-u.ac.jp)... 130.54.208.131\n",
            "Connecting to lotus.kuee.kyoto-u.ac.jp (lotus.kuee.kyoto-u.ac.jp)|130.54.208.131|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 132762852 (127M) [application/x-gzip]\n",
            "Saving to: ‘indic_languages_corpus.tar.gz’\n",
            "\n",
            "indic_languages_cor 100%[===================>] 126.61M  12.9MB/s    in 11s     \n",
            "\n",
            "2022-11-03 22:21:05 (11.0 MB/s) - ‘indic_languages_corpus.tar.gz’ saved [132762852/132762852]\n",
            "\n",
            "time: 12.3 s (started: 2022-11-03 22:20:53 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4AxU0WlbZSx",
        "outputId": "adf41977-84c6-41a9-f087-ce047afe5356"
      },
      "source": [
        "import tarfile\n",
        "with tarfile.open('indic_languages_corpus.tar.gz', 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "print(\"done!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n",
            "time: 3.69 s (started: 2022-11-03 22:21:05 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So4-SXJYbtMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657c601f-1613-4f02-94a9-e4c1c974c6e1"
      },
      "source": [
        "#We copy the Hindi to English files for working in this example (dev.en, dev.hi, test.en, test.hi and train.en, train.hi)\n",
        "%cp indic_languages_corpus/bilingual/hi-en/* .\n",
        "#Clean up to avoid storing these files in the session\n",
        "%rm -r indic_languages_corpus indic_languages_corpus.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 272 ms (started: 2022-11-03 22:21:09 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h_y0wbzcxSs",
        "outputId": "86a3a34a-3bbd-41f9-a0eb-06d6168af933"
      },
      "source": [
        "# understanding how the training data looks like\n",
        "f = open('train.hi')\n",
        "w1 = f.readlines()\n",
        "print(len(w1))\n",
        "print(w1[0:5])\n",
        "g = open('train.en')\n",
        "w2 = g.readlines()\n",
        "print(len(w2))\n",
        "print(w2[0:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84557\n",
            "['और उनके Sigil क्या है?\\n', 'मैं मरना नहीं चाहता.\\n', 'यह मुझे लगता है कि एक ही देश है.\\n', 'फिर ये नन्हें बच्चों की तरह रोएँगे।\\n', 'नहीं, मुझे पावर की जरुरत है !\\n']\n",
            "84557\n",
            "['And what is their Sigil?\\n', 'I do not want to die.\\n', \"It's the same country I think.\\n\", \"Then they'll be crying like babies.\\n\", '- No, I need power up!\\n']\n",
            "time: 57.4 ms (started: 2022-11-03 22:21:09 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL_IvHeamlhc"
      },
      "source": [
        "# **Data Preperation**\n",
        "\n",
        "Once we have loaded the dataset, we preprocess the data as follows:\n",
        "\n",
        "Add a start and end token to each sentence.\n",
        "\n",
        "Clean the sentences by removing special characters.\n",
        "\n",
        "Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "\n",
        "Pad each sentence to a maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLiTegnlgyvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9f3874-e231-4e0d-f637-ee0f71e370b4"
      },
      "source": [
        "# Restrict the total number of sentences to 70000\n",
        "NUM_SENTENCES = 70000"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 876 µs (started: 2022-11-03 22:21:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmsVNqzYdjxo",
        "outputId": "ba5b05d1-775a-417d-91da-65807ba92c73"
      },
      "source": [
        "# strip the input and output of extra unnecessary characters\n",
        "# store all the cleaned input and output sentences into input_sentences[] and output_sentences[]\n",
        "# tokenize the Hindi (target) sentences using the indicNLP libary class and add <sos> (start-of-sentence) and <eos> (end-of-sentence)\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "\n",
        "count = 0\n",
        "for line in open(r'train.en', encoding=\"utf-8\"):\n",
        "    count += 1\n",
        "\n",
        "    if count > NUM_SENTENCES:\n",
        "        break\n",
        "\n",
        "    input_sentence = line.rstrip().strip(\"\\n\").strip('-') #we strip the sentence of '\\n' and '-' \n",
        "    input_sentences.append(input_sentence) #store all input sentences in the input sentences list\n",
        "\n",
        "count = 0\n",
        "\n",
        "for line in open(r'train.hi'):\n",
        "    count += 1\n",
        "\n",
        "    if count > NUM_SENTENCES:\n",
        "        break\n",
        "    output_sentence =  line.rstrip().strip(\"\\n\").strip('-') \n",
        "    from indicnlp.tokenize import indic_tokenize  \n",
        "    line = indic_tokenize.trivial_tokenize(output_sentence) #we tokenize the hindi sentences \n",
        "    \n",
        "    output_sentences.append(['<sos>'] + line + ['<eos>']) #append the start and end tags to the tokenised sentences\n",
        "                                                          #each tokenied sentence is stored as a list in output sentences\n",
        "print(type(input_sentences[10]))\n",
        "print(type(output_sentences[10]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "<class 'list'>\n",
            "time: 1.2 s (started: 2022-11-03 22:21:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2kB9RH2fBE0",
        "outputId": "50c06053-fd8f-476d-deae-ef437a1dd91f"
      },
      "source": [
        "print(\"num samples input:\", len(input_sentences))\n",
        "print(\"num samples output:\", len(output_sentences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num samples input: 70000\n",
            "num samples output: 70000\n",
            "time: 1.19 ms (started: 2022-11-03 22:21:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkMgkiSnhIQE",
        "outputId": "cbadbce1-a85b-4bd3-ae34-14bd7ff2d7c4"
      },
      "source": [
        "print(input_sentences[-1])\n",
        "print(output_sentences[-1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Her face.\n",
            "['<sos>', 'उसका', 'चेहरा', '.', '<eos>']\n",
            "time: 1.03 ms (started: 2022-11-03 22:21:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_PsNxfbSvle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5c7c55-aa30-4a32-9f6c-14f0a0b03a22"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "# Since the model is dealing with multilingual text so it will be important to standardize the input text.\n",
        "# Unicode normalization splits accented characters and replace compatibility characters with their ASCII equivalents.\n",
        "# https://bit.ly/2TnLffX\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.3 ms (started: 2022-11-03 22:21:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NTM7NQ_R8fT",
        "outputId": "b86869d5-bf30-4cdd-ca5a-567b6945794e"
      },
      "source": [
        "for i in range(len(input_sentences)):\n",
        "   input_sentences[i] = preprocess_sentence(input_sentences[i])\n",
        "\n",
        "print(input_sentences[8])\n",
        "print(output_sentences[8])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos> i told her we rest on sundays . <eos>\n",
            "['<sos>', 'मैं', 'रविवार', 'को', 'उसे', 'हम', 'बाकी', 'बताया', '.', '<eos>']\n",
            "time: 1.23 s (started: 2022-11-03 22:21:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM3aEpCUhxh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e41a63-e292-48c3-dde2-64d743325960"
      },
      "source": [
        "# function to tokenize, fit the words into numeric sequences and pad them with zeroes up to the size of the largest sentence of that vocabulary\n",
        "# takes as input the input / output vocabulary and the padding type ('pre' / 'post'-- default: post)\n",
        "\n",
        "# inp_lang and targ_lang is of type tokenizer.fit_on_texts; \n",
        "# fit_on_texts of Tokenizer class updates internal vocabulary based on a list of texts. \n",
        "# This method creates the vocabulary index based on word frequency. \n",
        "# Lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
        "\n",
        "def tokenize(lang, pad): \n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  \n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  \n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.58 ms (started: 2022-11-03 22:21:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgsojarlh81H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208d6250-13cf-47d9-c027-bcf1d3528c35"
      },
      "source": [
        "# function to call the tokenize function to perform tokenizing and padding\n",
        "\n",
        "def load_dataset(inp_lang, targ_lang):\n",
        "  # creating cleaned input, output pairs\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang, 'post')\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang, 'post')\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.79 ms (started: 2022-11-03 22:21:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FxxPJYFi3cX",
        "outputId": "af13d739-c81d-40cb-effb-fcd8078485a9"
      },
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(input_sentences, output_sentences)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "# For our project, the max_length_targ and max_length_inp are 69 and 72 respectively.\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "print(max_length_targ)\n",
        "print(max_length_inp)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69\n",
            "72\n",
            "time: 3.95 s (started: 2022-11-03 22:21:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blHpIh2WTR7M",
        "outputId": "deef3d1b-90ca-439e-dbd1-7d942cb329dc"
      },
      "source": [
        "# checking if the input sequences have been obtained and padded properly\n",
        "print(target_tensor[9])\n",
        "print(input_tensor[9])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1  47 203  18 203  26  39 553  79  29   5 270   8   2   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "[   1    5  106   62   63  462 6235   21    4   59    8    2    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "time: 9.45 ms (started: 2022-11-03 22:21:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao4xP-xckEhE",
        "outputId": "2246f8cc-f1a0-4aff-b42b-f564081edde6"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2,random_state=7)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56000 56000 14000 14000\n",
            "time: 48.8 ms (started: 2022-11-03 22:21:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2gSXFB7DNI9",
        "outputId": "d156f8ad-7808-4511-b14f-3bdbdf3faafb"
      },
      "source": [
        "# checking if the input sequences have been obtained and padded properly\n",
        "print(input_tensor_val[9])\n",
        "print(target_tensor_val[9])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    1    60   451    14    60 10406    99   207   135     8     2     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
            "[   1   30  161  147  484    4    5   19 4339   31  747  692    4    8\n",
            "    2    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "time: 1.63 ms (started: 2022-11-03 22:21:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy0Z2a0bl9QB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6154cd6c-5f68-4057-9612-77812a47d871"
      },
      "source": [
        "# a function to test if the word to index / index to word mappings have been obtained correctly. \n",
        "# representative output for two sample english and hindi sentences given in the code block below\n",
        "\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "      print (\"%s ----> %d\" % (lang.index_word[t], lang.word_index[lang.index_word[t]]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.38 ms (started: 2022-11-03 22:21:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmJypgD3l3P6",
        "outputId": "c0e1042a-069a-4ae7-d73b-344ebf44bbb3"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <sos>\n",
            "<sos> ----> 1\n",
            "74 ----> did\n",
            "did ----> 74\n",
            "5 ----> you\n",
            "you ----> 5\n",
            "2270 ----> threaten\n",
            "threaten ----> 2270\n",
            "21 ----> me\n",
            "me ----> 21\n",
            "8 ----> ?\n",
            "? ----> 8\n",
            "2 ----> <eos>\n",
            "<eos> ----> 2\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <sos>\n",
            "<sos> ----> 1\n",
            "15 ----> आप\n",
            "आप ----> 15\n",
            "26 ----> मुझे\n",
            "मुझे ----> 26\n",
            "1426 ----> धमकी\n",
            "धमकी ----> 1426\n",
            "45 ----> किया\n",
            "किया ----> 45\n",
            "29 ----> था\n",
            "था ----> 29\n",
            "8 ----> ?\n",
            "? ----> 8\n",
            "2 ----> <eos>\n",
            "<eos> ----> 2\n",
            "time: 9.6 ms (started: 2022-11-03 22:21:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCSWpdefmNaS",
        "outputId": "a9582b46-25c7-49bc-c86c-0daebcbcf256"
      },
      "source": [
        "# BUFFER_SIZE stores the number of training points\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "\n",
        "# BATCH_SIZE is set to 64. Training and gradient descent happens in batches of 64\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# the number of batches in one epoch (also, the number of steps during training, when we go batch by batch)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "\n",
        "# the length of the embedded vector\n",
        "embedding_dim = 256\n",
        "\n",
        "# no of GRUs\n",
        "units = 1024 \n",
        "\n",
        "# getting the size of the input and output vocabularies.\n",
        "vocab_inp_size = len(inp_lang.word_index)+1 \n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "# now, we shuffle the dataset and split it into batches of 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # the remainder after splitting by 64 are dropped\n",
        "\n",
        "print(BUFFER_SIZE)\n",
        "print(BUFFER_SIZE//64)\n",
        "print(steps_per_epoch)\n",
        "print(max_length_targ)\n",
        "print(max_length_inp)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56000\n",
            "875\n",
            "875\n",
            "69\n",
            "72\n",
            "time: 3.95 s (started: 2022-11-03 22:21:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2_KlgAMm-ZK",
        "outputId": "10b4a3b6-8d1e-4745-d792-3c76cf299be7"
      },
      "source": [
        "# to understand the shape of an input batch\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 72]), TensorShape([64, 69]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 297 ms (started: 2022-11-03 22:21:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IJ35HtdpH9P"
      },
      "source": [
        "# **Encoder-Decoder model**\n",
        "\n",
        "The encoder model consists of an embedding layer, a GRU layer with 1024 units.\n",
        "\n",
        "The decoder model consists of a embedding layer, a GRU layer and a dense layer.\n",
        "\n",
        "---\n",
        "![picture](https://drive.google.com/uc?id=1BjzsnC-lcn4GapfGv1hUDDyb68ySS4cW)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yyyfnUqqCSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29dadf38-cf65-4e50-a717-f53ff98a9d34"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz # set batch size\n",
        "    self.enc_units = enc_units # set the number of GRU units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # set the embedding layer using the input's vocabulary size and the embedding dimension (which is set to 256)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform') # define the GRU layer\n",
        "\n",
        "  def call(self, x, hidden): # this function is invoked when the function encoder is called with an input and an initialised hidden layer\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden) # pass input x into the GRU layer\n",
        "    return output, state # function returns the encoder output and the hidden state\n",
        "\n",
        "\n",
        "  def initialize_hidden_state(self): #intialise hidden layer to all zeroes (for determining the shape)\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.3 ms (started: 2022-11-03 22:21:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo28Qbfa0rIX",
        "outputId": "bb593684-7281-4a19-bf98-191398de8c4e"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE) # create an Encoder class object\n",
        "\n",
        "# sample input to get a sense of the shapes.\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 72, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "time: 3.62 s (started: 2022-11-03 22:21:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEMbTmGW9VT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37dbf5a2-159d-44f2-e50e-bbba0632e00d"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz # batch_size which is defined as 64\n",
        "    self.dec_units = dec_units # the number of decoder GRU units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # defining an embedding layer for the target language output. \n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform') # GRU layer\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x) # creating an embedding layer for the target output\n",
        "\n",
        "    # passing the initial state to the GRU as the hidden state\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output) # pass the output through the dense layer\n",
        "\n",
        "    return x, state # return decoder output and decoder state "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.76 ms (started: 2022-11-03 22:21:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1wvr3Ie9X81",
        "outputId": "e252fa3e-d0d3-4361-dbb4-1fc5b2233b39"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 22224)\n",
            "time: 2.01 s (started: 2022-11-03 22:21:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEniteeTHbjH"
      },
      "source": [
        "# **Training the model**\n",
        "\n",
        "The model is trained on a GPU machine with fixed number of epochs. \n",
        "\n",
        "A custom training loop (instead of Model.Fit etc.) is used for which further reference is available from Tensorflow [here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)\n",
        "\n",
        "The model can be extended with the use of the validation data for early stopping and further fine tuning. \n",
        "\n",
        "Checkpoints are stored for easy retrieval of the model and resue without training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erDnU7mw9acE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57cca51-1f5b-4467-e450-d6eb5f9bcb11"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none') #Loss function is categorical crossentropy\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.05 ms (started: 2022-11-03 22:21:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ryWGXER9goc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25edf3e5-5ec0-48e4-de6b-0b2603455767"
      },
      "source": [
        "checkpoint_dir = './tutorial_checkpoint_nmt'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.98 ms (started: 2022-11-03 22:21:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBBj4-9U9r1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3807f9d4-0315-450b-96be-59b4151d974a"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables) \n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables)) # doing gradient descent\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.76 ms (started: 2022-11-03 22:21:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF7cJDaHeMkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71085b22-aa27-4a0d-c404-ad5fde3c846f"
      },
      "source": [
        "train = False\n",
        "EPOCHS = 10\n",
        "if train :\n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      batch_loss = train_step(inp, targ, enc_hidden)\n",
        "      total_loss += batch_loss\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                    batch,\n",
        "                                                    batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.5 ms (started: 2022-11-03 22:21:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwgSqCLf_Tht",
        "outputId": "e62c5077-d8a4-4970-89fe-989b923e2a23"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7fd088debad0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.74 ms (started: 2022-11-03 22:21:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAFPeCr66Wm"
      },
      "source": [
        "# **Prediction using Greedy Search**\n",
        "\n",
        "Greedy search is used to for Decoding of text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPkwfe6E_Csj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66146be6-51f2-4398-909c-5aa0f318c0e4"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden = decoder(dec_input,dec_hidden)\n",
        "    \n",
        "    # pass the encoder output, decoder hidden state(which is initialised to encoder hidden state for the first time and decoder input to the decoder)\n",
        "    # make a prediction and obtain decoder hidden states\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.92 ms (started: 2022-11-03 22:21:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exQRehVR_RR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8aba38-1c7e-4ddc-bf56-b9e28477f39e"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  return result"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.17 ms (started: 2022-11-03 22:21:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "qHcnO2JbFrp7",
        "outputId": "24d92101-b768-40b4-8798-706a72a1e4b5"
      },
      "source": [
        "translate(\"i am hungry\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <sos> i am hungry <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 478 ms (started: 2022-11-03 22:21:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "QOFXsr5EanzX",
        "outputId": "41359331-f3b4-46a2-dce7-bd592ced7bf8"
      },
      "source": [
        "translate(\"I am hungry. Can you give me something to eat.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <sos> i am hungry . can you give me something to eat . <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 439 ms (started: 2022-11-03 22:21:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tgoxk2BcdBy"
      },
      "source": [
        "# **Calculating BLEU score for evaluation**\n",
        "\n",
        "BLEU score (Bilingual Evaluation Understudy) is calculated on the test data for evaluating the quality of translations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4hD3TgDcQ1k",
        "outputId": "35df2ca2-770d-48c8-b095-86614e699fbf"
      },
      "source": [
        "test_input_sentences = []\n",
        "test_output_sentences = []\n",
        "\n",
        "for line in open(r'test.en', encoding=\"utf-8\"):\n",
        "\n",
        "    test_input_sentence = line.rstrip().strip(\"\\n\").strip('-')\n",
        "    test_input_sentences.append(test_input_sentence)\n",
        "\n",
        "\n",
        "for line in open(r'test.hi'):\n",
        "    test_output_sentence =  line.rstrip().strip(\"\\n\").strip('-')\n",
        "    line = indic_tokenize.trivial_tokenize(test_output_sentence)\n",
        "    \n",
        "    test_output_sentences.append(['<sos>'] + line + ['<eos>'])\n",
        "    \n",
        "print(type(test_input_sentences[90]))\n",
        "print(len(test_output_sentences))\n",
        "print(test_input_sentences[90])\n",
        "print(test_output_sentences[90])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "1000\n",
            "You're slower than molasses in January.\n",
            "['<sos>', 'आप', 'जनवरी', 'में', 'गुड़', 'की', 'तुलना', 'में', 'धीमी', 'है', '.', '<eos>']\n",
            "time: 22.2 ms (started: 2022-11-03 22:21:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbaONTQ_cah7",
        "outputId": "f7620070-cb83-419c-b7df-e17699d8f946"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "chencherry = SmoothingFunction()\n",
        "evaluate_n_sentences = 10\n",
        "\n",
        "references = []\n",
        "candidates = []\n",
        "for i in range(evaluate_n_sentences):\n",
        "  try:\n",
        "    res = translate(test_input_sentences[i]) \n",
        "    ref = test_output_sentences[i].copy()\n",
        "    ref = [e for e in ref if e not in ('<eos>', '<sos>', '.')]\n",
        "    references.append(ref)\n",
        "    listToStr = ' '.join(map(str, test_output_sentences[i]))\n",
        "    print('Reference Translation: %s' % (listToStr))\n",
        "    candidate = indic_tokenize.trivial_tokenize(res)\n",
        "    candidate = [e for e in candidate if e not in ('<', 'eos','>', '.')]\n",
        "    candidates.append(candidate)\n",
        "  except:\n",
        "    print('Sentence :', i+1, ' not translatable ..moving to next' )\n",
        "score1 = corpus_bleu(references, candidates, smoothing_function=chencherry.method4)\n",
        "score2 = corpus_bleu(references, candidates)\n",
        "print('BLEU score on test data without smoothing function: ' ,score2)\n",
        "print('BLEU score on test data with smoothing function: ' ,score1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence : 1  not translatable ..moving to next\n",
            "Input: <sos> storm will be the closest man to him . <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n",
            "Reference Translation: <sos> तूफान उसे निकटतम आदमी हो जाएगा . <eos>\n",
            "Input: <sos> well , ilse , now you have to eat something , too . <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n",
            "Reference Translation: <sos> खैर , इल्से , अब तुम्हें भी कुछ खाना है . <eos>\n",
            "Sentence : 4  not translatable ..moving to next\n",
            "Input: <sos> your smile is saying it loudly , my dear . . . <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n",
            "Reference Translation: <sos> Your smile is saying it loudly , my dear . . . <eos>\n",
            "Input: <sos> hey , remember my games of hangman ? <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n",
            "Reference Translation: <sos> अरे , जल्लाद का मेरे खेल याद है ? <eos>\n",
            "Input: <sos> no , i just work here in sales . <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n",
            "Reference Translation: <sos> नहीं , मैं बिक्री में काम करता हूँ । <eos>\n",
            "Input: <sos> just trying to make it easier on you . <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n",
            "Reference Translation: <sos> सिर्फ तुम पर आसान बनाने के लिए कोशिश . <eos>\n",
            "Input: <sos> do you know what that is , kids ? <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n",
            "Reference Translation: <sos> तुम्हें पता है वह क्या है , बच्चों ? <eos>\n",
            "Input: <sos> i take it that you re interested in . <eos>\n",
            "Predicted translation: मौजूद भिजवाऊँगा मलदार हेरोदेस परम किशोरी untraind पिघलाव andras तेज़ी ऐलिस हैंक pushpakh उर्फ शुभकामनाएं अगली शत वेफल्स चिप्स निंदा प्रथम महिलारिपोर्टर बदकिस्मत मज़ाक़ महबूबेह मर सिमॉनजॉनसन नहींसर अगवा adios सकतीं परिस्थितियों मेराएक लाइफ़ पकड़नी तोड़ना लाए जैकबेहतरउन फ़ॉन्ट try जेनिस लीवर ाय मार्टीनी सुदूरपूर्वमेंछह फासीवादी स्वीकार वेह्रमैच फेंक मामिहलापिनातापई विशेषज्ञ डरने बादहमारीपूरी थीम जोड़ी शाही खोजकर यंग्ज़हौ प्रोब गाते बैरबस स्ट्रिंग कराना èoporu giving करीब royce royce वाला \n",
            "Reference Translation: <sos> मैं आप में रुचि रखते हैं कि इसे ले जाओ । <eos>\n",
            "BLEU score on test data without smoothing function:  0\n",
            "BLEU score on test data with smoothing function:  0\n",
            "time: 3.97 s (started: 2022-11-03 22:21:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_rAhbB7GlIq"
      },
      "source": [
        "# *References*\n",
        "\n",
        "1. The dataset used is available from [here](http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/) \n",
        "\n",
        "2. Refer the tensorflow tutorials available on NMT [here](https://tensorflow.org/tutorials/text/nmt_with_attention) and [here](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt) for examples on which this notebook is modelled. \n",
        "\n",
        "3. Refer reference code and documentation available [here](https://github.com/prashanthi-r/Eng-Hin-Neural-Machine-Translation) which has been adopted\n",
        "\n",
        "4. Indic Library documentation can be found [here](https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
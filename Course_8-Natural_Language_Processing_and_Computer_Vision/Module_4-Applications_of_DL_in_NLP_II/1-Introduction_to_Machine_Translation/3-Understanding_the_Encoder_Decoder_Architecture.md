# Understanding the Encoder-Decoder Architecture

Before proceeding further, let’s take a pause and play a translation game.   
In the next video, Mohit will introduce you to the rules of the game and how it is played.

**VIDEO**

As explained in the video, the teacher passes a set of words in English to a group of students belonging to a particular class. Each student intakes the new word passed by the teacher along with the information received by the preceding student. The student, after receiving both the information, whispers the message into the next student’s ear. Once the set of words are finished, the teacher ends the communication by sending a stop signal to the student at the end of this group.   
It is this student’s responsibility to pass the entire information to a student of another class. The job of the new class is to intake this information and provide the necessary translation in Hindi. **The entire information acts as the context for this new class** but each student of this class will provide only one word as part of the translation. Once a student produces a translated word, the next student hears it and provides the next word as part of the entire translation. This continues until a student provides a stop signal, which halts the translation. **Thus, the entire set of words provided by the new class acts as the translation for the input set provided by the teacher.**   
Here, the first group of students together acts as the encoder for the game and the decoder is played by the second class who provides us the required translation.   
This translation game is analogous to the **sequence to sequence (seq2seq) model**, where each student can be represented as a part of a Recurrent Neural Network (RNN). 

  
The next video will help you in connecting the dots between the game and the seq2seq model.

**VIDEO**

Sequence-to-sequence (seq2seq) model follows an encoder-decoder architecture, where it is made up of two RNN’s. Both the encoder & decoder consists of a series of RNN cells where each layer’s output is the input to the next layer. 

![Encoder-Decoder](https://i.ibb.co/HKkFf6f/Encoder-Decoder.png)

## Encoder

The encoder RNN takes the input sequence and encodes it into a fixed size context vector. It does the job by reading the input tokens one at a time. The **context vector** generated by the final layer’s hidden state is then fed to the decoder as the input. 

One of the attractive features of the seq-2-seq model is its ability to turn a sequence of words into a vector of fixed dimensionality (context vector). The below plot shows some of the learned representations of the input sequence. The phrases used in the plot are clustered as per their meaning (which also captures their word order).  

![The figure shows a 2-dimensional PCA projection of the hidden states,  obtained after processing the phrases](https://i.ibb.co/hK5zNys/PCA-Projection.png)

The figure shows a 2-dimensional PCA projection of the hidden states, obtained after processing the phrases .

## Decoder

The decoder RNN uses this context vector to generate the output sequence. The first cell of the decoder is initialised with the hidden state that it received from the encoder. The decoder here acts as a **language model** as it predicts the next word based on the previous prediction and the hidden step passed from the cell at the previous time step. 

The entire seq2seq/NMT model is called a **Conditional Language model**.

-   **Conditional model** as the decoder’s prediction is based on the context input (condition) that it has received from the encoder.
-   **Language model** as the decoder is predicting the next word based on the previous prediction. 

**NOTE**: A language model estimates the unconditional probability of a sequence p(yt|yt−1)  but a seq-2-seq model estimates the conditional probability p(yt|yt−1,x)of a sequence given a source. 

To understand the decoding process, let's assume that the input sentence is represented by  (x1,x2,…,xn) and the target is represented by (y1,y2,....,yn). At a timestep t, the model produces a probability distribution p(∗|y1,y2,......,yt−1,x1,x2,…,xn).

![Illustration of how the decoder generates a probability distribution at each timestep](https://i.ibb.co/74QkFcY/Decoder-Generates-a-Probability-Distribution.gif)

Illustration of how the decoder generates a probability distribution at each timestep.

You must have noticed that once the input tokens are read by the encoder, we pass a special token to the encoder **`<stop>/<end>`**. This token indicates the encoder **to stop encoding** and pass the last layer’s hidden state to the decoder. The special token can be named as **`<eos>` (end of sentence)** or anything else as it is just an indicator for the model’s convenience.   
Along with the context vector, the decoder receives a special token as well. In this case, the **`<start>/<sos>` (start of sentence)** token **indicates the model to start decoding**. Once it has provided the relevant translation, it will generate an **`<end>/<eos>`** token **indicating the end of the target sentence.**

#### Special Tokens

Qn: What is the need for **`<start>/<sos>`** & **`<end>/<eos>`** tokens? _(More than one option may be correct)_

- They are the added tokens in the input and output data for making the input/output sequence uniform.

They are the prompters/indicators to the sequence to sequence model while training and prediction.

**`<start>/<sos>`** token is added to indicate the decoder to start prediction and **`<end>/<eos>`** token is added to prompt the decoder in stopping the predictions. 

Ans: B & C. *They help in prompting encoder and decoder during training and prediction. `<start>/<sos>` token prompts the decoder to start prediction while the `<end>/<eos>` token prompts the decoder to stop the predictions.*

#### Sequence to Sequence models

Qn: The applications of the sequence to sequence model include \_\_\_\_\_. Fill in the blank with the correct answer from the options given below. _(More than one option may be correct)_

- Machine translation 

- Question and answering systems

- Image captioning 

- All of the above

Ans: D. *Sequence to sequence models can help in understanding different types of paired data like language pairs, question-answer, image-caption etc.*

Qn: The context vector output of the encoder in the traditional sequence to sequence model \_\_\_\_\_.  Fill in the blank with the correct answer from the options given below.

- represents the translation of the entire sentence

- is also the softmax of the output vectors

- represents the encoded input sentence as a vector

Ans: C. *The encoder RNN takes the input sequence and encodes it into a fixed size context vector.*

Based on input and output, you can understand that the NMT architecture can handle variable length of input and output. To produce an efficient translation, the NMT model should satisfy certain conditions which we will talk about in the next segment – Requirements of NMT Architecture.
# Introduction

In this session, you will study two important variants of the RNN that have made it possible to train large networks on real data sets. In the previous session, you saw that although RNNs are capable of solving a variety of sequence problems, their architecture itself is their biggest enemy. You learnt about the problems of exploding and vanishing gradients that occur during the training of RNNs. In this session, you will learn how to solve this problem with the help of two popular gated RNN architectures: the Long Short-Term Memory (LSTM) and the Gated Recurrent Unit (GRU).

**In this session**, the following topics will be covered:

1.  LSTM overview
2.  LSTM architecture and gating mechanism
3.  LSTM model training
4.  GRU architecture
5.  Comparison among RNN, GRU and LSTM

## Youâ€™ll be hearing from

**Mouli Sankaran**  
**Visiting Professor, CS Department, IIIT Lucknow**  
Professor Mouli is currently working as a visiting faculty at IIIT Lucknow, handling courses on AI for IoT and Python. He has a total of 30+ years of experience in networking, embedded software, machine learning with 24+ years of industrial experience in defining, designing, and delivering high-quality products & systems and 6+ years in teaching. He has made his name in both academics as well as industry and that has always ensured that he is above the bar whenever he delivers the content.
# Summary

In this session, you learnt about some commonly used variants of RNNs.

You learnt about long short-term memory networks (or LSTMs). You learnt about the three gates of an LSTM cell and how they modify the cell state or the memory.

You also learnt about the structure of an LSTM cell and its feed-forward equations. The number of parameters in an LSTM layer is four times the number of parameters in a standard RNN.

Finally, you learnt about some other variants of LSTM such as GRUs which has 1 lesser gate in comparison to the former. The number of parameters in a GRU layer is three times the number of parameters in a standard RNN layer.

In the next segment, you will attempt some graded questions of this session.
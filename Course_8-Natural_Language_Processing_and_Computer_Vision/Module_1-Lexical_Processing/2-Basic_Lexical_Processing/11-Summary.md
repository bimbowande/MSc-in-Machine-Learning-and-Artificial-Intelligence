# Summary

In this session, you learnt a lot of essential preprocessing steps that you would need to apply when working with a corpus of text. First, you learnt about text encoding and the encoding standards. Post that, you learnt how to use regular expressions to remove punctuation marks and emojis from text.

Then, you gained an understanding of stop words, which are words that add no information in certain applications, such as the spam detector. Further, you learnt how to remove English stop words using NLTK’s list of stopwords.

  
Next, you went through the process of tokenising a document. You learnt that a document can be tokenised based on words, sentences, paragraphs or your own custom regular expression.  
Then, you understood the importance of removing redundant words from the corpus using two techniques: stemming and lemmatization. You learnt that stemming converts a word to its root from by chopping off its suffix, while lemmatization reduces a word to its base form, called the lemma, by going through the WordNet library. You also got a fair idea of the advantages and disadvantages of each technique.

#### Summary

Qn: What are your top three key takeaways from this session? Word Count **0**Word Limit **10 - 100**

Ans: *Suggested Answer*

1.  You learnt how to remove noise from a text corpus. 
    
2.  You gained an understanding of stop words and Zipf’s law, Following which you went through a demonstration of removing stop words. 
    
3.  You learnt about stemming and lemmatization as methods to combine different variations of the same word in the corpus.

In the next segment, you will attempt some graded questions.